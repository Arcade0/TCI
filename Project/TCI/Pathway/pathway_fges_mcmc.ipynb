{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import javabridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     1,
     7,
     50,
     61,
     75
    ]
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "def mk_dir(file_path):\n",
    "\n",
    "    folder = os.path.exists(file_path)\n",
    "    if not folder:\n",
    "        os. makedirs(file_path)\n",
    "\n",
    "def EM(S_Am, S_Dm, A_D, fix_mutation=0, e=1e-5, cut_value=0.85, ite=10000, cover_thres=1e-4):\n",
    "\n",
    "    S_Ad = S_Am.values\n",
    "    S_Dd = S_Dm.values\n",
    "    A_Dd = A_D.values\n",
    "    logpD1A1_l = [1]\n",
    "\n",
    "    for i in range(ite):\n",
    "\n",
    "        pA1 = (np.sum(S_Ad, 0)[:, None]+1)/(S_Ad.shape[0]+2)\n",
    "        pD1A1 = (np.dot(S_Ad.T, S_Dd) + 1)/(np.sum(S_Ad, 0)[:, None]+2)\n",
    "        pD1A0 = (np.dot(1-S_Ad.T, S_Dd)+1)/(np.sum(1-S_Ad, 0)[:, None]+2)\n",
    "\n",
    "        logpA1 = np.log(pA1 + e)\n",
    "        logpA0 = np.log(1-pA1 + e)\n",
    "        logpD1A1 = np.log(pD1A1 + e) * A_Dd\n",
    "        logpD0A1 = np.log(1-pD1A1 + e) * A_Dd\n",
    "        logpD1A0 = np.log(pD1A0 + e) * A_Dd\n",
    "        logpD0A0 = np.log(1-pD1A0 + e) * A_Dd\n",
    "\n",
    "        logpA1D = np.dot(S_Dd, logpD1A1.T) + \\\n",
    "            np.dot(1 - S_Dd, logpD0A1.T) + logpA1.T\n",
    "        logpA0D = np.dot(S_Dd, logpD1A0.T) + \\\n",
    "            np.dot(1 - S_Dd, logpD0A0.T) + logpA0.T\n",
    "        S_Ad = 1 / (1 + np.exp(logpA0D - logpA1D))\n",
    "\n",
    "        if fix_mutation == 1:\n",
    "            S_Ad = S_Ad+S_Am\n",
    "            S_Ad[S_Ad >= 1] = 1\n",
    "            S_Ad[S_Ad > cut_value]=1\n",
    "            S_Ad[S_Ad <= cut_values]=0\n",
    "        else:\n",
    "            S_Ad = S_Ad\n",
    "\n",
    "        logpD1A1_l.append(logpD1A1)\n",
    "        if np.mean((logpD1A1_l[i] - logpD1A1_l[i-1])**2 )< cover_thres:\n",
    "            break\n",
    "\n",
    "    S_Ad0 = pd.DataFrame(S_Ad, index=S_Am.index, columns=S_Am.columns)\n",
    "    S_Ad1 = deepcopy(S_Ad0)\n",
    "    para = [logpA1, logpA0, logpD1A1, logpD0A1, logpD1A0, logpD0A0]\n",
    "    return S_Ad0, S_Ad1, para\n",
    "\n",
    "def create_TAD(df):\n",
    "\n",
    "    df['cause gene name'] = df.index\n",
    "    TAD = pd.melt(df, id_vars='cause gene name',\n",
    "                  value_vars=list(df.columns[0:-1]),\n",
    "                  var_name='result gene name', value_name='value')\n",
    "    TAD1 = TAD[TAD['value'] == 1]\n",
    "    TAD0 = TAD[TAD['value'] == 0]\n",
    "\n",
    "    return TAD, TAD1, TAD0\n",
    "\n",
    "def combine_S_AD(S_Am, S_Dm, TAD1, SGA_l):\n",
    "\n",
    "    S_ADm = pd.concat((S_Am, S_Dm), axis=1)\n",
    "\n",
    "    nTAD1 = TAD1[TAD1['cause gene name'].isin(SGA_l)]\n",
    "    DEG_l = list(np.unique(nTAD1['result gene name']))\n",
    "\n",
    "    SGA_l = [ele for ele in SGA_l if ele in S_Am.columns]\n",
    "    DEG_l = [ele.replace('.', '-') for ele in DEG_l]\n",
    "    nS_Am = S_Am[SGA_l]\n",
    "    nS_ADm = S_ADm[SGA_l+DEG_l]\n",
    "\n",
    "    return nS_Am, nS_ADm\n",
    "\n",
    "def create_knowledge(SGA, SGA_l, A_D):\n",
    "\n",
    "    A_D['cause gene name'] = A_D.index\n",
    "    TAD = pd.melt(A_D, id_vars='cause gene name', value_vars=list(A_D.columns[0:-1]), var_name='result gene name', value_name='value')\n",
    "    TAD1 = TAD[TAD['value'] == 1]\n",
    "    TAD0 = TAD[TAD['value'] == 0]\n",
    "\n",
    "    SGA.columns = ['cause gene name']\n",
    "    TAD1_SGA = pd.merge(SGA, TAD1, 'inner').iloc[:, [0, 1]]\n",
    "    TAD0_SGA = pd.merge(SGA, TAD0, 'inner').iloc[:, [0, 1]]\n",
    "    forbid_l = TAD0_SGA.values.tolist()\n",
    "\n",
    "    return forbid_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "def fges_stem(file_path, sys_iter, SGA_l, A_D):\n",
    "\n",
    "    BIC_l = [float(0)]\n",
    "\n",
    "    SGA = pd.DataFrame(SGA_l)\n",
    "    SGA.columns = ['cause gene name']\n",
    "    A_D_i = A_D\n",
    "\n",
    "    for i in range(sys_iter):\n",
    "        print(i)\n",
    "        file_l = os.listdir(file_path + '/Output/run%i' % i)\n",
    "        while 'completeMatrixn.csv' not in file_l:\n",
    "            df_name = file_path + '/Output/run%i/completeMatrix.csv' % i\n",
    "            df = pd.read_csv(df_name, header=0, index_col=None)\n",
    "\n",
    "            from pycausal.pycausal import pycausal as pc\n",
    "            pc = pc()\n",
    "            pc.start_vm(java_max_heap_size='5000M')\n",
    "\n",
    "            from pycausal import prior as p\n",
    "            # get knowledge from knowledge file\n",
    "            # prior = p.knowledgeFromFile(file_path + '/Input/Knowledge')\n",
    "            \n",
    "            # get knowledge from DEG and SGA list\n",
    "            DEG_l = [x for x in df.columns if x not in SGA_l]\n",
    "            A_D_i = A_D_i[DEG_l]\n",
    "            forbid = create_knowledge(SGA, SGA_l, A_D_i)\n",
    "            temporal = [SGA_l, p.ForbiddenWithin(DEG_l)]\n",
    "            prior = p.knowledge(forbiddirect=forbid, addtemporal=temporal)\n",
    "\n",
    "            from pycausal import search as s\n",
    "            tetrad = s.tetradrunner()\n",
    "            tetrad.getAlgorithmParameters(algoId='fges', scoreId='bdeu')\n",
    "\n",
    "            tetrad.run(algoId='fges', dfs=df, scoreId='bdeu', priorKnowledge=prior, dataType='discrete', structurePrior=1.0, samplePrior=1.0,\n",
    "                       maxDegree=100, faithfulnessAssumed=True, verbose=True)  # , numberResampling=10, resamplingEnsemble=1, addOriginalDataset=True)\n",
    "\n",
    "            # save edges.csv\n",
    "            node_l = tetrad.getNodes()\n",
    "            edge_l = tetrad.getEdges()\n",
    "            #edge_split_l = []\n",
    "            #for edge in edge_l:\n",
    "                #if '---' in edge:\n",
    "                    #edge_n = edge.split(' ')\n",
    "                    #if np.sum(df[edge.split(' ')[0]]) > np.sum(df[edge.split(' ')[2]]):\n",
    "                    #    edge_n.reverse()\n",
    "                    #else:\n",
    "                    #    edge_n = edge_n\n",
    "                    #edge_split_l.append(edge_n)\n",
    "                #else:\n",
    "                    #edge_split_l.append(edge.split(' '))\n",
    "\n",
    "            #edge_split_l = [edge.split(' ') for edge in edge_l if '---' not in edge]\n",
    "            edge_split_l = [edge.split(' ') for edge in edge_l]\n",
    "\n",
    "            edge_df = pd.DataFrame(edge_split_l).iloc[:, [0, 2]]\n",
    "            edge_df.to_csv(file_path + '/Output/run%i/Edge.csv' %i, index=False, header=False)\n",
    "            \n",
    "            edge_split_sl = [ele for ele in edge_split_l if '--- sga' in ele or '--> sga' in ele]\n",
    "            edge_sdf = pd.DataFrame(edge_split_sl).iloc[:, [0, 2]]\n",
    "            edge_sdf.to_csv(file_path + '/Output/run%i/sEdge.csv' %i, index=False, header=False)\n",
    "\n",
    "\n",
    "            # save completeMatrixn.csv\n",
    "            new_df = df.loc[:, node_l]\n",
    "            new_df.to_csv(file_path + '/Output/run%i/completeMatrixn.csv' %i, index=False, header=True)\n",
    "\n",
    "            # save BIC.txt\n",
    "            print(tetrad.getTetradGraph(), file=open(file_path + '/Output/run%i/BIC.txt' % i, 'a'))\n",
    "            file_l = os.listdir(file_path + '/Output/run%i' % i)\n",
    "\n",
    "        else:\n",
    "            # save BIC which used to verify convergency\n",
    "            with open(file_path+'/Output/run%i/BIC.txt' % i, 'r') as BIC_txt:\n",
    "                for line in BIC_txt:\n",
    "                    if 'BIC: -' in line:\n",
    "                        BIC_l.append(float(line[5:-1]))\n",
    "\n",
    "            j = i+1\n",
    "            mk_dir(file_path + '/Output/run%d' % j)\n",
    "            next_file_l = os.listdir(file_path + '/Output/run%i' % j)\n",
    "            while 'completeMatrix.csv' not in next_file_l:\n",
    "                exe_path = './MCMC/inferSGAInNetwork_TDI.exe'\n",
    "                m_path = ' -m ' + file_path + '/Output/run%i/completeMatrixn.csv' % i\n",
    "                i_path = ' -i ' + file_path + '/Input/S_A0.csv'\n",
    "                e_path = ' -e ' + file_path + '/Output/run%i/Edge.csv' % i\n",
    "                o_path = ' -o ' + file_path + '/Output/run%d/ -x 50' % j\n",
    "                combine = exe_path + m_path + i_path + e_path + o_path\n",
    "                os.system(combine)\n",
    "                time.sleep(20)\n",
    "                next_file_l = os.listdir(file_path + '/Output/run%i' % j)\n",
    "            else:\n",
    "                pd.DataFrame(BIC_l).to_csv(file_path+'/Output/BIC.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     3,
     7
    ]
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "\n",
    "\n",
    "RB1_l = ['CDKN1A', 'CDKN1B', 'CDKN2A', 'CDKN2B', 'CDKN2C', 'CCNE',\n",
    "         'CCND1', 'CCND2', 'CCND3', 'CDK2', 'CDK4', 'CDK6', 'RB1', 'E2F1', 'E2F3']\n",
    "RBl_l = ['CDKN2A', 'CDK4','RB1','E2F3']\n",
    "RTK_l = ['EGFR', 'ERBB2', 'ERBB4', 'MET', 'PDGFRA', 'FGFR1', 'FGFR2', 'FGFR3', 'FGFR4', 'KIT', 'IGF1R', 'RET', 'ROS1', 'ALK', 'FLT3', 'NTRK1-3', 'JAK2', 'CBL', 'ERRFI1','ABL1', 'SOS1', 'NF1', 'RASA1', 'PTPN11', 'KRAS', 'HRAS', 'NRAS', 'RIT1', 'ARAF', 'BRAF', 'RAF1', 'RAC1', 'MAPK1', 'MAP2K1', 'MAP2K2']\n",
    "PI3K_l = ['PTEN', 'PIK3R2', 'PIK3R1', 'PIK3R2', 'PIK3CA', 'PIK3CB', 'INPP4B',\n",
    "          'AKT1', 'AKT2', 'AKT3', 'TSC1', 'TSC2', 'RHEB', 'STK11', 'MTOR', 'RICTOR', 'RPTOR']\n",
    "NOTCH_l = ['FBXW7','NOTCH1','EP300','CREBBP']\n",
    "\n",
    "S_Amrn = pd.read_csv('Pre/S_Amrn.csv', header=0, index_col=0)\n",
    "ALL_l = list(S_Amrn.columns)\n",
    "RB1_l = ['sga:'+ele for ele in RB1_l]\n",
    "RBl_l = ['sga:'+ ele for ele in RBl_l]\n",
    "RTK_l = ['sga:'+ele for ele in RTK_l]\n",
    "PI3K_l = ['sga:'+ele for ele in PI3K_l]\n",
    "NOTCH_l = ['sga:'+ele for ele in NOTCH_l]\n",
    "SM7_l = ['sga:'+str(i) for i in range(7)]\n",
    "SM15_l = ['sga:'+str(i) for i in range(15)]\n",
    "\n",
    "\n",
    "SGA_d = {'ALL': ALL_l,'RB1': RB1_l,'RBl':RBl_l,'RTK': RTK_l, 'PI3K': PI3K_l,'NOTCH': NOTCH_l, 'SM7':SM7_l, 'SM15':SM15_l}\n",
    "#####################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     9
    ]
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "S_Am = pd.read_csv(str(sys.argv[1]), header=0, index_col=0)\n",
    "S_Dm = pd.read_csv(str(sys.argv[2]), header=0, index_col=0)\n",
    "A_D = pd.read_csv(str(sys.argv[3]), header=0, index_col=0)\n",
    "\n",
    "if str(sys.argv[4]) == 'EM':\n",
    "\n",
    "    S_Ad0, S_Ad1, para = EM(S_Am, S_Dm, A_D, fix_mutation=0, e=1e-5, cut_value=0.85, ite=10000, cover_thres=1e-3)\n",
    "    S_Ad1.to_csv(str(sys.argv[5]), header=True, index=True)\n",
    "if str(sys.argv[4]) == 'noEM':\n",
    "    S_Ad1 = pd.read_csv(str(sys.argv[5]),header=0, index_col=0)\n",
    "\n",
    "TAD, TAD1, TAD0 = create_TAD(A_D)\n",
    "SGA_l = SGA_d[str(sys.argv[6])]\n",
    "nS_Am, nS_ADm = combine_S_AD(S_Am, S_Dm, TAD1, SGA_l)\n",
    "nS_Ad, nS_ADd = combine_S_AD(S_Ad1, S_Dm, TAD1, SGA_l)\n",
    "\n",
    "input_path = str(sys.argv[7]) + '/Input'\n",
    "output_path = str(sys.argv[7]) + '/Output'\n",
    "mk_dir(input_path)\n",
    "mk_dir(output_path+'/run0')\n",
    "nS_Am.to_csv(input_path+'/S_A0.csv', header=True, index=False)\n",
    "nS_ADd.to_csv(output_path+'/run0'+'/completeMatrix.csv', header=True, index=False)\n",
    "\n",
    "fges_stem(str(sys.argv[7]), int(sys.argv[8]), SGA_l, A_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Aom = pd.read_csv(\"EM/Tetrad Input/Results.csv\", index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycausal.pycausal import pycausal as pc\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size='5000M')\n",
    "\n",
    "from pycausal import prior as p\n",
    "# get knowledge from knowledge file\n",
    "prior = p.knowledgeFromFile('EM/Tetrad Input/Knowledge copy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
